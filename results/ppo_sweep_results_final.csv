config_name,algorithm,training_time,mean_reward,std_reward,mean_food_saved,mean_food_wasted,mean_avg_quality,mean_efficiency
ppo_tight_clip,PPO,422.854446,118.98039852631405,11.073294810073353,3.1819120138725276,0.21232225586931688,0.09560078189060942,93.67982342920374
ppo_deep,PPO,500.960084,118.8906451931382,5.485019827287517,2.8539281821808538,0.130884754683739,0.12468137245143325,95.9544849351574
ppo_baseline,PPO,391.648842,116.94007792132557,5.16040722960709,3.1106612697519953,0.22264968567025786,0.14281622058508786,93.1083889443593
ppo_entropy,PPO,392.929967,116.62432612540799,10.683513830446282,3.2890707847989136,0.23303235137010464,0.11605345052751692,93.3085479265687
ppo_wide,PPO,449.101979,116.40382538926781,13.570585303190505,3.0921723862352217,0.23326685067542302,0.10662745893831835,92.69408290723968
ppo_many_epochs,PPO,523.570796,116.12498718914887,9.437369396207087,3.168506691053169,0.23491809011217163,0.10378671179986887,92.81331422989817
ppo_long_horizon,PPO,360.4636,114.85723732528723,7.8666984435134735,3.050741654461464,0.2372884729782673,0.11777422036372735,92.54949058149961
ppo_aggressive,PPO,401.336067,112.80854083807394,14.066722023191799,3.2436202110727805,0.22351248096428783,0.10724573303328364,93.49581346136355
ppo_large_batch,PPO,295.033744,111.39914273451186,17.86095308947947,3.1300039389752894,0.25288760444726255,0.11296174183686902,92.04977264866636
ppo_high_lr,PPO,329.712907,108.27456903923849,10.715293231593714,3.1129971649823487,0.22300111727116775,0.14344741901824307,93.0724549683384
